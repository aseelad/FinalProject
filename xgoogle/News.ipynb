{
 "metadata": {
  "name": "",
  "signature": "sha256:feac6cad6358ba212b2a3799240120675ef7b2d5f5b8a8511c8c15699b0c7d01"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## get URL form google serach and write it to a text file\n",
      "from xgoogle.search import GoogleSearch, SearchError\n",
      "\n",
      "Newspapers=[\"nytimes.com\",\"champress.net\",\"timesofindia.indiatimes.com\", \"aawsat.net\", \"english.people.com.cn\" ,\"bbc.co.uk\" ,\"tehrantimes.com\"\n",
      "            ,\"dw.de\",\"thejakartapost.com\",\"reuters.com\"]\n",
      "for news in Newspapers:\n",
      "    \n",
      "    try:\n",
      "        gs = GoogleSearch(\"Gay rights site:\"+news)\n",
      "        gs.results_per_page = 100\n",
      "        results = []\n",
      "        while True:\n",
      "            tmp = gs.get_results()\n",
      "            if not tmp: # no more results were found\n",
      "                break\n",
      "            results.extend(tmp)\n",
      "\n",
      "        f = open(\"GayRights/\"+news+\".txt\", \"w\")\n",
      "        for res in results:\n",
      "            f.write(res.title.encode('utf8'))\n",
      "            f.write(res.url.encode('utf8'))\n",
      "            f.write(\"\\n\")\n",
      "\n",
      "        f.close()\n",
      "    except SearchError, e:\n",
      "      print \"Search failed: %s\" % e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##preporcessing\n",
      "##cleaning the data\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "import re\n",
      "import string\n",
      "import os\n",
      "import unicodedata\n",
      "\n",
      "\n",
      "##get text file\n",
      "Newspapers=[\"nytimes.com\",\"thejakartapost.com\",\"reuters.com\"]\n",
      "for news in Newspapers:\n",
      "    tokenized_docs =[]\n",
      "    rootdir =\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"/\"\n",
      "    for subdir, dirs, files in os.walk(rootdir):\n",
      "        for fi in files:\n",
      "            if fi == '.DS_Store':\n",
      "                continue  # skip the file\n",
      "            u= os.path.join(subdir, fi)\n",
      "            f = open(u, 'r').read()\n",
      "            tokenized_docs.extend(word_tokenize(f.decode('utf-8')))\n",
      "    ##Removing punctuation\n",
      "    regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
      "    tokenized_docs_no_punctuation = []\n",
      "    for token in tokenized_docs: \n",
      "        new_token = regex.sub(u'', token)\n",
      "        if not new_token == u'':\n",
      "            tokenized_docs_no_punctuation.append(new_token)\n",
      "\n",
      "    \n",
      "    ##remove stop words\n",
      "    tokenized_docs_no_stopwords = []\n",
      "    for word in tokenized_docs_no_punctuation:\n",
      "        if not word in stopwords.words('english'):\n",
      "            tokenized_docs_no_stopwords.append(word)\n",
      "    \n",
      "    \n",
      "    ##write the update version to a file\n",
      "    f = open(rootdir+\"_Updated.txt\", \"w\")\n",
      "    for word in tokenized_docs_no_stopwords:\n",
      "        word=unicodedata.normalize('NFKD', word).encode('ascii','ignore')\n",
      "        f.write(str(word)+\" \")\n",
      "        #f.write(\"\\n\")\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##do TF-IDF REFER BACK TO TF-IDF.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##get the count of all words in each newspapers\n",
      "import nltk\n",
      "import string\n",
      "\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "Newspapers=[\"nytimes.com\",\"thejakartapost.com\",\"reuters.com\"]\n",
      "for news in Newspapers:\n",
      "    rootdir =\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"/\"\n",
      "    words=[]\n",
      "    for subdir, dirs, files in os.walk(rootdir):\n",
      "        for fi in files:\n",
      "            if fi == '.DS_Store':\n",
      "                continue  # skip the file\n",
      "            u= os.path.join(subdir, fi)\n",
      "            f = open(u, 'r')\n",
      "            text = f.read()\n",
      "            lowers = text.lower()\n",
      "            #remove the punctuation using the character deletion step of translate\n",
      "            no_punctuation = lowers.translate(None, string.punctuation)\n",
      "            tokens =nltk.word_tokenize(no_punctuation)\n",
      "            words.extend(tokens)\n",
      "    count = Counter(words)\n",
      "\n",
      "    filtered = [w for w in words if not w in stopwords.words('english')]\n",
      "    count = Counter(filtered)\n",
      "    most_common= count.most_common(20)\n",
      "    #save to file\n",
      "    f = open(\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"_most_common.txt\", \"w\")\n",
      "    for item in most_common:\n",
      "        f.write(str(item))\n",
      "        f.write(\"\\n\")\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:27: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## get the average length of all article for each newspaper\n",
      "\n",
      "# read all articles\n",
      "Newspapers=[\"nytimes.com\",\"thejakartapost.com\",\"reuters.com\"]\n",
      "for news in Newspapers:\n",
      "    rootdir =\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"/\"\n",
      "    Counting=[]\n",
      "    for subdir, dirs, files in os.walk(rootdir):\n",
      "        for fi in files:\n",
      "            if fi == '.DS_Store':\n",
      "                continue  # skip the file\n",
      "            u= os.path.join(subdir, fi)\n",
      "            f = open(u, 'r')\n",
      "            text = f.read()\n",
      "            lowers = text.lower()\n",
      "            #remove the punctuation using the character deletion step of translate\n",
      "            no_punctuation = lowers.translate(None, string.punctuation)\n",
      "            tokens = nltk.word_tokenize(no_punctuation)\n",
      "            count= len(tokens)\n",
      "            #get the count of words\n",
      "            Counting.append(int(count))\n",
      "        #get the average \n",
      "        sum=0\n",
      "        average=0\n",
      "        for n in Counting:            \n",
      "            sum = sum + n\n",
      "        average = sum / len(Counting)\n",
      "        #save to file\n",
      "        f = open(\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"_average.txt\", \"w\")\n",
      "        f.write(str(average))\n",
      "        f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###get the total number of articles for each newspaper\n",
      "\n",
      "# read all articles\n",
      "Newspapers=[\"nytimes.com\",\"thejakartapost.com\",\"reuters.com\"]\n",
      "for news in Newspapers:\n",
      "    rootdir =\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"/\"\n",
      "    count=0\n",
      "    for subdir, dirs, files in os.walk(rootdir):\n",
      "        for fi in files:\n",
      "            if fi == '.DS_Store':\n",
      "                continue  # skip the file\n",
      "            count=count+1    \n",
      "    #save to file\n",
      "    f = open(\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"_ArticleCount.txt\", \"w\")\n",
      "    f.write(str(count))\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from __future__ import division\n",
      "import urllib\n",
      "import csv\n",
      "from string import punctuation\n",
      "\n",
      "##read postitve words\n",
      "pos_sent = open(\"positive.txt\").read()\n",
      "positive_words=pos_sent.split('\\n')\n",
      "\n",
      "##read negative words\n",
      "neg_sent = open('negative.txt').read()\n",
      "negative_words=neg_sent.split('\\n')\n",
      "\n",
      "# read all articles\n",
      "Newspapers=[\"nytimes.com\",\"thejakartapost.com\",\"reuters.com\"]\n",
      "for news in Newspapers:\n",
      "    rootdir =\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"/\"\n",
      "    count=0\n",
      "    W=[]\n",
      "    positive_counts=[]\n",
      "    negative_counts=[]\n",
      "    for subdir, dirs, files in os.walk(rootdir):\n",
      "        for fi in files:\n",
      "            if fi == '.DS_Store':\n",
      "                continue  # skip the file\n",
      "            u= os.path.join(subdir, fi)\n",
      "            f = open(u, 'r')\n",
      "            text = f.read()\n",
      "            lowers = text.lower()\n",
      "            #remove the punctuation using the character deletion step of translate\n",
      "            no_punctuation = lowers.translate(None, string.punctuation)\n",
      "            tokens = nltk.word_tokenize(no_punctuation)\n",
      "            W.extend(tokens)\n",
      "\n",
      "        positive_counter=0\n",
      "        negative_counter=0\n",
      "    \n",
      "        word_count=len(W)\n",
      "        for word in W:\n",
      "            if word in positive_words:\n",
      "                positive_counter=positive_counter+1\n",
      "            elif word in negative_words:\n",
      "                negative_counter=negative_counter+1\n",
      "        \n",
      "        positive_counts=positive_counter/word_count\n",
      "        negative_counts=negative_counter/word_count\n",
      "        \n",
      "        if positive_counts > negative_counts:\n",
      "            Sentmenint=\"Pos \" + str(positive_counts)\n",
      "        \n",
      "        if positive_counts < negative_counts:\n",
      "            Sentmenint=\"Neg\" + str(negative_counts)\n",
      "        \n",
      "        if positive_counts == negative_counts:\n",
      "            Sentmenint=\"Natural\" + str(positive_counts)\n",
      "        #save to file\n",
      "        f = open(\"/Users/Aseel/xgoogle/islamicstate/\"+news+\"_Sentmenint.txt\", \"w\")\n",
      "        f.write(str(Sentmenint))\n",
      "        f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}